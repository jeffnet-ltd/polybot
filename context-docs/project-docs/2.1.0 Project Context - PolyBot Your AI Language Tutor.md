Version: 2.1.0 (Stable, Verified, Production Ready)

Status: Production Ready - Core Features Complete + Full A1 Curriculum (10 Modules) + GPTQ Quantization + Azure-Only TTS Stack + Gendered Voice Support

Last Updated: 2025-12-11 (v2.1.0 - Edge TTS Removed, Azure Speech Primary, Gendered Character Voices Implemented)

Dev Environment: HP EliteBook 820 G4 / Windows 11 WSL2 (NVIDIA RTX 2080 Ti - Docker Desktop with GPU Support) â†’ **Migrating to RunPod Serverless GPU**

## Executive Summary

Polybot is an **AI-powered multilingual language learning platform** that runs entirely locally â†’ **Migrating to a Serverless Cloud Architecture for scalability and global deployment.** It combines a structured **10-module CEFR A1 Curriculum** with free-flowing AI roleplay to provide a "True Bilingual" learning experience.

**Key Differentiator:** Users learn in their target language while receiving explanations in their native language. The architecture ensures total privacy and zero latency through local LLM use, which will be maintained in the cloud via **Serverless GPU Scaling**.

**Current Status:** **Complete A1 Curriculum (10 Modules)** - All modules A1.1 through A1.10 are fully implemented with comprehensive lesson structures, exercise types, and Boss Fight scenarios. Each module follows strict pedagogical patterns and vocabulary whitelists. Enhanced feedback mechanisms, voice integration (Whisper STT), and TTS-based listening comprehension are operational. **Scenario-Based Practice Mode architecture is designed and ready for implementation.** **GPTQ quantization successfully implemented for efficient model loading.** **Azure Speech Services is the sole TTS provider with gendered character voice support.**

---

## Technical Architecture (V2.1 - Current Stable)

### Stack Overview

- **Frontend:** React 18.2.0 + Tailwind CSS + Lucide React

    - **Deployment:** **Vercel / Netlify** (Free Tier)

- **Backend:** FastAPI (Python 3.11) + **Llama 3 8B Instruct (GPTQ Quantized)**

    - **Deployment:** **RunPod Serverless GPU** (Flex Workers for cost-efficiency)

- **Auth:** Google OAuth 2.0 (via `authlib`) + Local Session Management

- **Database:** MongoDB 7.0 â†’ **MongoDB Atlas (Free Tier)**

- **Testing:** `pytest` suite with `AsyncMock` for backend logic verification.

- **Infrastructure:** Docker + Docker Compose (V2) - **Migrating to Serverless Endpoints.**

### Model Quantization

**Status:** âœ… Implemented

- **Quantization Method:** GPTQ (4-bit quantization via `auto-gptq`)
- **Model:** `TheBloke/Llama-3-8B-Instruct-GPTQ`
- **VRAM Usage:** ~5.5GB (down from ~16GB for FP16)
- **Performance:** Maintains high quality with 3x memory reduction
- **Implementation:** Auto-detection via transformers' `AutoModelForCausalLM` when `auto-gptq` is installed
- **CUDA Extensions:** Optional CUDA extensions for improved performance (pre-built wheels for CUDA 11.8/12.1)

**Migration Notes:**
- Removed deprecated AutoAWQ (no longer maintained)
- Switched to actively maintained GPTQ solution
- Compatible with transformers >=4.40.0
- Graceful fallback to standard model loading if GPTQ unavailable

### Voice Integration (Implemented - Final Stack)

|**Component**|**Status**|**Model**|**Technical Justification**|
|---|---|---|---|
|**Speech-to-Text (STT)**|**âœ… Implemented**|**Whisper (OpenAI)**|Integrated for Echo Chamber exercises and Boss Fight voice input. Provides accurate transcription for pronunciation feedback.|
|**Text-to-Speech (TTS)**|**âœ… Implemented (Primary)**|**Azure Speech Services**|**Primary & sole TTS provider.** Provides high availability, consistent voice quality, and reliable performance. Replaces Edge-TTS entirely due to DNS instability in containerized environments.|
|**Gendered Character Voices**|**âœ… Implemented**|**Azure Neural Voices (Gender-Specific)**|**Character-aware voice selection** - Male and female voices mapped to character genders (Sofiaâ†’Female, Marcoâ†’Male, etc.) for enhanced learner immersion.|

### Hardware Requirement

- **AI Model:** Llama 3 8B Instruct (GPTQ quantized) will be hosted on **RunPod Serverless GPUs** (e.g., A40, L40S, or RTX 4090) using Docker containers.

- **Load Management:** **Model Ducking (VRAM Swapping)** and **GPTQ 4-bit Quantization** implemented to efficiently manage the concurrent VRAM load of Llama 3 and the Voice models (Whisper) on the shared GPU instance.

---

## Implemented Features (v2.1.0)

### Complete A1 Curriculum: 10 Modules

**Status:** âœ… All Modules Fully Implemented with Enhanced Exercise Types

#### Module A1.1: Greetings & Introductions
- **9 Complete Lessons:** Self-Assessment + 7 content lessons + Boss Fight
- **Grammar:** Subject Pronouns, Verb "Essere" (To Be), Nouns & Articles, Countries & Nationalities
- **Boss Fight:** Meeting a Neighbor (Informal & Formal)

#### Module A1.2: Personal Information & Family
- **8 Complete Lessons:** Self-Assessment + 6 content lessons + Boss Fight
- **Grammar:** "Avere" (To Have) for age/possession, "Essere" for descriptions, Numbers 0-20, Professions, Family vocabulary, Possessives
- **Boss Fight:** The Library Registration

#### Module A1.3: Home & Housing
- **8 Complete Lessons:** Self-Assessment + 6 content lessons + Boss Fight
- **Grammar:** "C'Ã¨" (There is) vs "Ci sono" (There are), Prepositions of place, Colors & Description
- **Boss Fight:** The Real Estate Viewing

#### Module A1.4: Ordering Food & Drinks
- **8 Complete Lessons:** Self-Assessment + 6 content lessons + Boss Fight
- **Grammar:** "Vorrei" (I would like) vs "Voglio", "Prendere" (To take), Plurals, Course structure
- **Boss Fight:** The Trattoria Dinner

#### Module A1.5: Shopping & Prices
- **8 Complete Lessons:** Self-Assessment + 6 content lessons + Boss Fight
- **Grammar:** Demonstratives (Questo/Questa - Singular), "Quanto", Verbs "Costare" and "Comprare"
- **Boss Fight:** The Market Haggle

#### Module A1.6: Directions & Transportation
- **8 Complete Lessons:** Self-Assessment + 6 content lessons + Boss Fight
- **Grammar:** Imperative mood (first steps), "Andare" (To go), Prepositions of movement, Relative positions
- **Boss Fight:** The Lost Tourist

#### Module A1.7: Time & Daily Routines
- **8 Complete Lessons:** Self-Assessment + 6 content lessons + Boss Fight
- **Grammar:** Present Simple (Regular -ARE verbs), Time expressions, Frequency adverbs, Daily routines
- **Boss Fight:** The Job Interview

#### Module A1.8: Weather & Seasons
- **8 Complete Lessons:** Self-Assessment + 6 content lessons + Boss Fight
- **Grammar:** Weather expressions, Seasons, "Fare" (To do/make) for weather, Clothing vocabulary
- **Boss Fight:** The Vacation Planning

#### Module A1.9: Hobbies & Interests
- **8 Complete Lessons:** Self-Assessment + 6 content lessons + Boss Fight
- **Grammar:** Sports/Activities verbs, "Piacere" (To like), Expressing preferences, Regular -ERE verbs
- **Boss Fight:** The Club Membership

#### Module A1.10: Health & Body Parts
- **8 Complete Lessons:** Self-Assessment + 6 content lessons + Boss Fight
- **Grammar:** Body parts vocabulary, Health expressions, "Avere" (To have) for ailments, Doctor/pharmacy interactions
- **Boss Fight:** The Pharmacy Consultation

---

## Recent Updates (v2.1.0)

### ðŸš€ Edge TTS Removal & Azure Speech Services Primary Implementation

**Status:** âœ… Completed on 2025-12-11

**What Changed:**
1. **Removed Edge-TTS Dependency**
   - Deleted Edge-TTS imports from `backend/server.py`
   - Removed `get_edge_tts_voice()` function
   - Removed diagnostic endpoints (`/api/v1/voice/tts-info`, `/api/v1/voice/test-tts`)
   - Deleted test files: `backend/test_tts.py`, `backend/scripts/generate_audio.py`
   - Removed `edge-tts` from `backend/requirements.txt`

2. **Azure Speech Services as Primary TTS Provider**
   - Simplified `synthesize_tts()` function to call Azure directly (no fallback logic needed)
   - Updated all 3 TTS endpoints to use Azure:
     - `/api/v1/voice/synthesize` - Dialogue exercises
     - `/api/v1/voice/chat` - Voice chat interactions
     - `/api/practice/voice-chat` - Practice mode voice responses
   - Performance improvement: **Reduced TTS latency by 1-5 seconds** (no Edge-TTS timeout attempt)

**Files Modified:**
- `backend/server.py` - Removed Edge-TTS imports, diagnostic endpoints, and updated TTS functions
- `backend/requirements.txt` - Removed `edge-tts` dependency
- `frontend/src/utils/audio.js` - Updated comments from "Edge-TTS" to "Azure Speech Services"

### ðŸŽ¤ Gendered Character Voice Support Implementation

**Status:** âœ… Completed on 2025-12-11

**New File: `backend/character_voices.py`**

A maintainable configuration system for character-to-voice mapping:

**Character-Gender Mapping:**
```python
CHARACTER_GENDERS = {
    # Male characters
    "Marco": "male",
    "Luca": "male",
    "John": "male",

    # Female characters
    "Sofia": "female",
    "Maria": "female",
    "Luisa": "female",
    "Anna": "female",
}
```

**Voice Pairs by Language (High-Quality Neural Voices):**

|Language|Male Voice|Female Voice|
|---|---|---|
|**English**|en-US-GuyNeural|en-US-JennyNeural|
|**Italian**|it-IT-DiegoNeural|it-IT-ElsaNeural|
|**French**|fr-FR-HenriNeural|fr-FR-DeniseNeural|
|**Spanish**|es-ES-AlvaroNeural|es-ES-ElviraNeural|
|**Portuguese**|pt-BR-AntonioNeural|pt-BR-FranciscaNeural|
|**German**|de-DE-ConradNeural|de-DE-KatjaNeural|
|**Japanese**|ja-JP-KeitaNeural|ja-JP-NanamiNeural|
|**Chinese**|zh-CN-YunxiNeural|zh-CN-XiaoxiaoNeural|

**Key Functions:**
- `get_character_gender(character_name)` - Returns gender based on character name
- `get_voice_for_character(lang_code, character_name)` - Selects appropriate voice based on language + gender
- `extract_character_name(text)` - Parses dialogue text to extract character names (e.g., "Marco: Ciao!" â†’ "Marco")

**Implementation in TTS:**
1. **`synthesize_azure_speech()`** - Enhanced to accept optional `character_name` parameter
2. **`synthesize_tts()`** - Wrapper that passes character info to Azure Speech
3. **Endpoints Updated:**
   - `/api/v1/voice/synthesize` - Extracts character name from dialogue and uses gendered voice
   - `/api/v1/voice/chat` - System responses use default female voice
   - `/api/practice/voice-chat` - System responses use default female voice

**How It Works:**
- When processing "Marco: Ciao!", the system:
  1. Extracts "Marco" as the character name
  2. Looks up Marco â†’ "male" gender
  3. Selects the male voice for the target language
  4. Marco's dialogue is spoken by a male voice

**Future Enhancements:**
- Simply add new characters to `CHARACTER_GENDERS` in `character_voices.py`
- No code changes needed - system automatically uses appropriate voice

---

## Roadmap: Next Steps (Priority)

1. **âœ… TTS High Availability (Completed in v2.1.0):**

    **1.1 Azure Speech Services as Primary Provider:**

    - **âœ… Done:** Removed Edge-TTS entirely due to DNS instability in containerized environments.
    - **âœ… Done:** Implemented Azure Speech Services as the sole, reliable TTS provider.
    - **âœ… Done:** Simplified TTS architecture (no fallback logic needed).

    **1.2 Gendered Character Voice Support:**

    - **âœ… Done:** Created character-gender mapping system in `character_voices.py`.
    - **âœ… Done:** Implemented gender-specific voice selection based on character names.
    - **âœ… Done:** Enhanced learner immersion through character-appropriate voices.
    - **Future:** Add more characters as curriculum expands.

2. **Streaming Pipeline Implementation:**

    - Implement streaming text generation using `TextIteratorStreamer` from transformers
    - Parse LLM output sentence-by-sentence (split on periods)
    - Stream complete sentences immediately to TTS as they're generated
    - Stream TTS audio chunks back to client as they're produced
    - Target: Reduce perceived latency from ~6 seconds to ~1.5 seconds

3. **Scenario-Based Practice Mode Implementation:**

    - Implement Game State Architecture in FastAPI backend

    - Create Stage Manager Pattern system prompts

    - Implement Goal Check Classifier with JSON response parsing

    - Build Post-Game Report feedback system

    - Develop Voice/Text Mode toggle UI

    - Create PushToTalkButton component

    - Design scenario library (e.g., "Ordering Coffee", "Buying Train Ticket")
    - Replace the current free tutor mode with this new mode and design

4. **âœ… Core Curriculum Complete:** All **10-Module A1 Course** content is pedagogically correct and complete. All modules (A1.1 through A1.10) are fully implemented with structured lesson data, exercise types, and Boss Fight scenarios.

5. **Cloud Migration ("The Hub"):**

    - **Backend:** Deploy Dockerized FastAPI to **RunPod Serverless GPU**.

    - **Database:** Migrate local MongoDB to **MongoDB Atlas (Free Tier)**.

    - **Frontend:** Deploy React app to **Vercel/Netlify**.

6. **Mobile App Conversion:** Package the React frontend into a native container using **Capacitor**.

    - Use `npx cap add android` and `npx cap add ios` to create native projects pointing to the cloud backend (The Hub).

7. **Code Cleanup:** Resolve Pydantic V2 and FastAPI `lifespan` deprecation warnings identified during testing.

8. **Global Expansion (Multi-Language Support):**

    **Architecture:** Support every language on earth while only paying for one single GPU using MultiLLMManager logic.

    **Summary Checklist for Global Expansion:**

    - [ ] **Add Storage:** Attach a 50GB Network Volume to your RunPod instance.
    - [ ] **Download Models:**
        - [ ] Llama-3-8B (Euro) - For European languages (Italian, Spanish, French, German, etc.)
        - [ ] Qwen-2.5-7B (Asian) - For Asian languages (Chinese, Japanese, Korean, etc.)
        - [ ] Aya-Expanse-8B (African) - For African languages and other global languages
    - [ ] **Update Backend:** Use the MultiLLMManager logic to swap models based on the user's selected language.

    **Technical Implementation:**
    - Model Manager class handles dynamic model loading/unloading based on user language selection
    - Language detection from user profile or explicit selection
    - VRAM management for efficient model swapping
    - Fallback logic if selected model unavailable
    - Language-specific curriculum modules (e.g., `a1_1_module_data_es.py` for Spanish, `a1_1_module_data_fr.py` for French)

---

## Technical Notes

### Model Quantization

- **GPTQ Implementation:** Uses `auto-gptq` library with transformers' auto-detection
- **Model Loading:** `AutoModelForCausalLM.from_pretrained()` automatically detects GPTQ format when `auto-gptq` is installed
- **CUDA Extensions:** Optional pre-built wheels for CUDA 11.8/12.1 improve performance
- **Fallback:** Gracefully falls back to standard model loading if GPTQ unavailable
- **Configuration:** Model name configurable via `MODEL_NAME` environment variable

### Practice Mode Architecture

- **Game State Management:** Conversation state tracked in backend with scene_status, win conditions, and conversation history
- **System Prompt Templates:** Structured prompts for Stage Manager Pattern stored in backend configuration
- **JSON Response Parsing:** Llama 3 Instruct configured to return structured JSON for scene_status detection
- **Feedback Generation:** Separate LLM calls with review-focused prompts for Post-Game Report generation
- **Mode Separation:** Frontend state management ensures Voice and Text modes never mix in UI

### Voice Stack Architecture (Final Implementation)

- **TTS Provider:** Azure Speech Services is the sole, reliable TTS provider.
- **Gendered Voices:** Character-aware voice selection system in `character_voices.py`
- **Character Recognition:** Automatic extraction of character names from dialogue text
- **Voice Mapping:** High-quality Azure Neural voices paired by gender for each language
- **System Responses:** Polybot AI uses default female voice for consistency
- **Performance:** No latency from failed TTS attempts (Azure is always available and reliable)

### Data Structure

- **Module Data:** Structured in `backend/a1_1_module_data.py` through `backend/a1_10_module_data.py` with hierarchical lesson/exercise organization
- **Exercise Validation:** Context-aware validation logic in `frontend/src/App.jsx` with fallback mechanisms
- **Boss Fight:** Static conversation flow with hard-coded messages and validation rules
- **Character Voices:** Configured in `backend/character_voices.py` with easy maintenance and expansion
- **New Exercise Fields:**
  - `audio_text`: Hidden sentence for listening comprehension (read by TTS, not displayed)
  - `common_mistakes`: Array of pattern-based mistake detection with explanations
  - `review`: Boolean flag for vocabulary review exercises
  - `extension`: Boolean flag for optional extension activities
  - `cultural_note`: Boolean flag for cultural information cards
  - `required_elements`: Array for free writing validation
  - `form_fields`: Array of field definitions for form fill exercises

### Key Files

**Core System:**
- `frontend/src/App.jsx` - Main React application with all exercise components
- `frontend/src/components/PushToTalkButton.jsx` - Voice Mode recording component (to be created)
- `backend/server.py` - FastAPI backend with lesson endpoints and voice processing
- `backend/practice_mode.py` - Scenario-based practice mode logic (to be created)
- `backend/scenario_templates.py` - System prompt templates for scenarios (to be created)

**Character & Voice Configuration:**
- `backend/character_voices.py` - **[NEW v2.1.0]** Character-gender mapping and voice selection logic

**Curriculum Data:**
- `backend/a1_1_module_data.py` through `backend/a1_10_module_data.py` - Complete A1 curriculum structured data (10 modules)
- `backend/scripts/seed_a1_1_module.py` through `backend/scripts/seed_a1_10_module.py` - MongoDB seeding scripts for all modules

**Configuration & Deployment:**
- `backend/requirements.txt` - Python dependencies (edge-tts removed in v2.1.0)
- `backend/Dockerfile` - Docker configuration with GPTQ CUDA extension installation
- `docker-compose.yml` - Docker Compose configuration with DNS settings for Azure Speech

**Documentation:**
- `context-docs/course-docs/The PolyBot A1 Curriculum Master Reference.md` - Curriculum reference
- `context-docs/project-docs/2.1.0 Project Context - PolyBot Your AI Language Tutor.md` - **This file**

---

## Version History

### v2.1.0 (Current - 2025-12-11)
- âœ… Removed Edge-TTS entirely, Azure Speech Services now primary & sole TTS provider
- âœ… Implemented gendered character voice support with `character_voices.py`
- âœ… Enhanced learner immersion through character-specific voices
- âœ… Improved TTS performance (1-5 second latency reduction)
- âœ… Streamlined TTS architecture (no fallback logic needed)
- âœ… Updated all documentation to reflect Azure-only approach

### v2.0.0 (Previous - 2025-01-XX)
- âœ… Complete A1 Curriculum (10 Modules: A1.1-A1.10)
- âœ… GPTQ Quantization implementation
- âœ… Azure Speech Services integration
- âœ… Voice integration (Whisper STT)
- âœ… Practice mode architecture design
- âœ… Cloud migration planning

---

## Notes for Future Development

1. **Easy Character Expansion:** When new characters are introduced to the curriculum, simply add them to `CHARACTER_GENDERS` in `character_voices.py`. The system will automatically use the appropriate gendered voice.

2. **Voice Quality Consistency:** All voices selected are Azure Neural voices at equivalent quality levels. Male and female voice pairs maintain consistent naturalness and professionalism.

3. **Azure Speech Reliability:** Azure Speech Services provides 99.9% SLA uptime. No DNS or containerization issues like Edge-TTS.

4. **Future TTS Considerations:** Azure Speech is the definitive TTS solution for PolyBot. No plans to add alternative TTS providers.

5. **Performance Monitoring:** Monitor Azure usage and costs post-deployment. Current usage patterns are minimal and well within Azure's free tier.
