Version: 1.0.15 (Stable, Verified, & Tested)

Status: Production Ready - Core Features Complete

Last Updated: 2025-12-01 (Updated with V2.0 Roadmap)

Dev Environment: HP EliteBook 820 G4 / Windows 11 WSL2 (NVIDIA RTX 2080 Ti - Docker Desktop with GPU Support) $\rightarrow$ **Migrating to RunPod Serverless GPU**

## Executive Summary

Polybot is an **AI-powered multilingual language learning platform** that runs entirely locally $\rightarrow$ **Migrating to a Serverless Cloud Architecture for scalability and global deployment.** It combines a structured **10-module CEFR A1 Curriculum** with free-flowing AI roleplay to provide a "True Bilingual" learning experience.

**Key Differentiator:** Users learn in their target language while receiving explanations in their native language. The architecture ensures total privacy and zero latency through local LLM use, which will be maintained in the cloud via **Serverless GPU Scaling**.

---

## Technical Architecture (V2.0 Target)

### Stack Overview

- **Frontend:** React 18.2.0 + Tailwind CSS + Lucide React
    
    - **Deployment:** **Vercel / Netlify** (Free Tier)
        
- **Backend:** FastAPI (Python 3.11) + **Llama 3 8B Instruct**
    
    - **Deployment:** **RunPod Serverless GPU** (Flex Workers for cost-efficiency)
        
- **Auth:** Google OAuth 2.0 (via `authlib`) + Local Session Management
    
- **Database:** MongoDB 7.0 $\rightarrow$ **MongoDB Atlas (Free Tier)**
    
- **Testing:** `pytest` suite with `AsyncMock` for backend logic verification.
    
- **Infrastructure:** Docker + Docker Compose (V2) - **Migrating to Serverless Endpoints.**
    

### Voice Integration (New V2.0 Components)

|**Component**|**Recommended Model**|**Technical Justification**|
|---|---|---|
|**Speech-to-Text (STT)**|**Whisper V3 Turbo (or Distil-Whisper)**|Best-in-class accuracy and speed for the currently supported, high-resource languages (EN, ES, FR, IT, PT, TW). Prioritizes low-latency conversion for smooth conversation.|
|**Text-to-Speech (TTS)**|**XTTS-v2 (Coqui)**|Free and open-source solution that provides **high-quality, expressive, multilingual synthesis**. Enables **zero-shot voice cloning** from a 6-second audio clip for a consistent AI Tutor voice across all target languages.|

### Hardware Requirement

- **AI Model:** Llama 3 8B Instruct will be hosted on **RunPod Serverless GPUs** (e.g., A40, L40S, or RTX 4090) using Docker containers.
    
- **Load Management:** **Model Ducking (VRAM Swapping)** and potential **4/8-bit Quantization** will be implemented to efficiently manage the concurrent VRAM load of Llama 3 and the Voice models (Whisper/XTTS-v2) on the shared GPU instance.
    

---

## Roadmap: Next Steps (Priority)

1. **Voice Integration (Priority):**
    
    - Integrate **Whisper** for Voice Input (STT) and **XTTS-v2** for AI Audio Output (TTS).
        
    - Implement GPU Load Management strategy (Model Ducking/Quantization) in the FastAPI backend.
        
2. **Core Curriculum Review:** Ensure all **10-Module A1 Course** content is pedagogically correct and complete before migration.
    
3. **Cloud Migration ("The Hub"):**
    
    - **Backend:** Deploy Dockerized FastAPI to **RunPod Serverless GPU**.
        
    - **Database:** Migrate local MongoDB to **MongoDB Atlas (Free Tier)**.
        
    - **Frontend:** Deploy React app to **Vercel/Netlify**.
        
4. **Mobile App Conversion:** Package the React frontend into a native container using **Capacitor**.
    
    - Use `npx cap add android` and `npx cap add ios` to create native projects pointing to the cloud backend (The Hub).
        
5. **Code Cleanup:** Resolve Pydantic V2 and FastAPI `lifespan` deprecation warnings identified during testing.