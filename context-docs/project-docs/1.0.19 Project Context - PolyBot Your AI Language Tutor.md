Version: 2.0.0 (Stable, Verified, & Tested)

Status: Production Ready - Core Features Complete + Full A1 Curriculum (10 Modules) + GPTQ Quantization + Voice Stack Migration Planned

Last Updated: 2025-01-XX (Updated to v2.0.0 with complete A1 curriculum modules A1.1-A1.10, Global Expansion roadmap)

Dev Environment: HP EliteBook 820 G4 / Windows 11 WSL2 (NVIDIA RTX 2080 Ti - Docker Desktop with GPU Support) $\rightarrow$ **Migrating to RunPod Serverless GPU**

## Executive Summary

Polybot is an **AI-powered multilingual language learning platform** that runs entirely locally $\rightarrow$ **Migrating to a Serverless Cloud Architecture for scalability and global deployment.** It combines a structured **10-module CEFR A1 Curriculum** with free-flowing AI roleplay to provide a "True Bilingual" learning experience.

**Key Differentiator:** Users learn in their target language while receiving explanations in their native language. The architecture ensures total privacy and zero latency through local LLM use, which will be maintained in the cloud via **Serverless GPU Scaling**.

**Current Status:** **Complete A1 Curriculum (10 Modules)** - All modules A1.1 through A1.10 are fully implemented with comprehensive lesson structures, exercise types, and Boss Fight scenarios. Each module follows strict pedagogical patterns and vocabulary whitelists. Enhanced feedback mechanisms, voice integration (Whisper STT), and TTS-based listening comprehension are operational. **Scenario-Based Practice Mode architecture is designed and ready for implementation.** **GPTQ quantization successfully implemented for efficient model loading.**

---

## Technical Architecture (V2.0 Target)

### Stack Overview

- **Frontend:** React 18.2.0 + Tailwind CSS + Lucide React
    
    - **Deployment:** **Vercel / Netlify** (Free Tier)
        
- **Backend:** FastAPI (Python 3.11) + **Llama 3 8B Instruct (GPTQ Quantized)**
    
    - **Deployment:** **RunPod Serverless GPU** (Flex Workers for cost-efficiency)
        
- **Auth:** Google OAuth 2.0 (via `authlib`) + Local Session Management
    
- **Database:** MongoDB 7.0 $\rightarrow$ **MongoDB Atlas (Free Tier)**
    
- **Testing:** `pytest` suite with `AsyncMock` for backend logic verification.
    
- **Infrastructure:** Docker + Docker Compose (V2) - **Migrating to Serverless Endpoints.**

### Model Quantization

**Status:** âœ… Implemented

- **Quantization Method:** GPTQ (4-bit quantization via `auto-gptq`)
- **Model:** `TheBloke/Llama-3-8B-Instruct-GPTQ`
- **VRAM Usage:** ~5.5GB (down from ~16GB for FP16)
- **Performance:** Maintains high quality with 3x memory reduction
- **Implementation:** Auto-detection via transformers' `AutoModelForCausalLM` when `auto-gptq` is installed
- **CUDA Extensions:** Optional CUDA extensions for improved performance (pre-built wheels for CUDA 11.8/12.1)

**Migration Notes:**
- Removed deprecated AutoAWQ (no longer maintained)
- Switched to actively maintained GPTQ solution
- Compatible with transformers >=4.40.0
- Graceful fallback to standard model loading if GPTQ unavailable

### Voice Integration (Implemented & Planned)

|**Component**|**Status**|**Model**|**Technical Justification**|
|---|---|---|---|
|**Speech-to-Text (STT)**|**âœ… Implemented**|**Whisper (OpenAI)**|Integrated for Echo Chamber exercises and Boss Fight voice input. Provides accurate transcription for pronunciation feedback.|
|**Text-to-Speech (TTS)**|**âœ… Implemented (Current)**|**Edge-TTS (Microsoft)**|Currently used for audio playback of vocabulary and phrases. Provides high-quality, natural-sounding synthesis. **Planned for removal in V2.0.**|
|**Text-to-Speech (TTS)**|**ðŸ”„ Planned (V2.0)**|**Piper TTS (CPU)**|Lightweight, fast CPU-based TTS for static content (vocabulary, phrases, exercise instructions). Runs efficiently without GPU resources.|
|**Text-to-Speech (TTS)**|**ðŸ”„ Planned (V2.0)**|**StyleTTS 2 (GPU)**|High-quality, expressive TTS for dynamic AI tutor responses. Enables **zero-shot voice cloning** from reference audio (5-10s clips) for consistent character voices. Supports dynamic voice switching (Old/Young/Male/Female) via `switch_voice(reference_wav)` function.|

### Hardware Requirement

- **AI Model:** Llama 3 8B Instruct (GPTQ quantized) will be hosted on **RunPod Serverless GPUs** (e.g., A40, L40S, or RTX 4090) using Docker containers.
    
- **Load Management:** **Model Ducking (VRAM Swapping)** and **GPTQ 4-bit Quantization** implemented to efficiently manage the concurrent VRAM load of Llama 3 and the Voice models (Whisper/StyleTTS 2) on the shared GPU instance.

---

## Implemented Features (v2.0.0)

### Complete A1 Curriculum: 10 Modules

**Status:** âœ… All Modules Fully Implemented with Enhanced Exercise Types

#### Module A1.1: Greetings & Introductions
- **9 Complete Lessons:** Self-Assessment + 7 content lessons + Boss Fight
- **Grammar:** Subject Pronouns, Verb "Essere" (To Be), Nouns & Articles, Countries & Nationalities
- **Boss Fight:** Meeting a Neighbor (Informal & Formal)

#### Module A1.2: Personal Information & Family
- **8 Complete Lessons:** Self-Assessment + 6 content lessons + Boss Fight
- **Grammar:** "Avere" (To Have) for age/possession, "Essere" for descriptions, Numbers 0-20, Professions, Family vocabulary, Possessives
- **Boss Fight:** The Library Registration

#### Module A1.3: Home & Housing
- **8 Complete Lessons:** Self-Assessment + 6 content lessons + Boss Fight
- **Grammar:** "C'Ã¨" (There is) vs "Ci sono" (There are), Prepositions of place, Colors & Description
- **Boss Fight:** The Real Estate Viewing

#### Module A1.4: Ordering Food & Drinks
- **8 Complete Lessons:** Self-Assessment + 6 content lessons + Boss Fight
- **Grammar:** "Vorrei" (I would like) vs "Voglio", "Prendere" (To take), Plurals, Course structure
- **Boss Fight:** The Trattoria Dinner

#### Module A1.5: Shopping & Prices
- **8 Complete Lessons:** Self-Assessment + 6 content lessons + Boss Fight
- **Grammar:** Demonstratives (Questo/Questa - Singular), "Quanto", Verbs "Costare" and "Comprare"
- **Boss Fight:** The Souvenir Shop

#### Module A1.6: Daily Routine & Time
- **8 Complete Lessons:** Self-Assessment + 6 content lessons + Boss Fight
- **Grammar:** Present Simple (Regular verbs), Reflexive Verbs (Mi/Ti/Si), Telling Time (Ãˆ l'una / Sono le...)
- **Boss Fight:** The Appointment

#### Module A1.7: Places in Town & Directions
- **8 Complete Lessons:** Self-Assessment + 6 content lessons + Boss Fight
- **Grammar:** Verb "Andare" (To go), The Imperative (Simple commands), Prepositions of movement (A, In)
- **Boss Fight:** The Lost Tourist

#### Module A1.8: Descriptions & Appearance
- **8 Complete Lessons:** Self-Assessment + 6 content lessons + Boss Fight
- **Grammar:** Adjective Agreement (Gender/Number), Verb "Piacere" (Mi piace vs Mi piacciono), Adverbs of degree
- **Boss Fight:** The Police Sketch / Dating Profile

#### Module A1.9: Travel & Transport
- **8 Complete Lessons:** Self-Assessment + 6 content lessons + Boss Fight
- **Grammar:** Verbs of Movement (Partire, Arrivare, Prendere), Prepositions of Transport (In vs A), Present Tense for Future Plans
- **Boss Fight:** The Ticket Office

#### Module A1.10: Health & Emergencies
- **8 Complete Lessons:** Self-Assessment + 6 content lessons + Boss Fight
- **Grammar:** Expression "Avere mal di..." (To have an ache), Irregular Body Plurals (Braccio/Braccia), Modal Verbs "Dovere" (Must) and "Potere" (Can)
- **Boss Fight:** The Medical Check

**Total Curriculum:** 80 lessons (10 self-assessments + 70 content lessons + 10 Boss Fights) covering complete CEFR A1 level Italian language learning.

### Exercise Types Implemented

|**Exercise Type**|**Status**|**Features**|
|---|---|---|
|**Info Cards**|âœ… Complete|Audio playback (Edge-TTS), vocabulary introduction, grammar explanations, cultural notes support|
|**Match Pairs**|âœ… Complete|Audio-to-text matching, Italian-English word pairs, interactive selection, state reset on exercise change, improved click handlers, vocabulary review mode|
|**Unscramble**|âœ… Complete|Drag-and-drop sentence construction, word order practice, comma normalization, common mistakes detection with enhanced feedback|
|**Echo Chamber**|âœ… Complete|Voice recording (MediaRecorder API), Whisper STT transcription, phonetic scoring (Levenshtein distance), playback before submission|
|**Mini-Prompt**|âœ… Complete|Contextual exercises, AI validation with fallback logic, context-specific feedback, accent detection (Italian), "almost" state for partial correctness, extension activities support|
|**Fill Blank**|âœ… Complete|Multiple choice fill-in-the-blank exercises|
|**Multiple Choice**|âœ… Complete|Standard multiple choice questions|
|**Gender Categorize**|âœ… Complete|Drag-and-drop noun gender classification (Maschile/Femminile), visual column layout|
|**Listening Comprehension**|âœ… Complete (NEW)|TTS-based audio playback with hidden sentences, multiple choice questions, replay functionality (up to max_plays), auto-play on mount|
|**Reading Comprehension**|âœ… Complete (NEW)|Text-based comprehension exercises with highlighted vocabulary, multiple choice questions, context-aware explanations|
|**Free Writing**|âœ… Complete (NEW)|Open-ended writing tasks with AI validation, required elements checking, example responses, context-based prompts|
|**Form Fill**|âœ… Complete (NEW)|Interactive form completion exercises with multiple field types (text, select), validation rules, hints|
|**Self-Assessment**|âœ… Complete (NEW)|Confidence-based assessment questions, skip option, no scoring, custom completion screen|
|**Boss Fight**|âœ… Complete|Static 2-round conversation system (Informal/Formal), hints tracking, grammar/spelling feedback, round transition controls|

### Enhanced Feedback System

- **Pedagogically-Focused Messages:** All exercises provide detailed explanations for correct/incorrect answers
- **"Almost" State:** Amber/warning feedback for partially correct answers (e.g., wrong accent, missing polite phrase)
- **Context-Specific Validation:** Mini-Prompt exercises use intelligent validation based on context (greetings, introductions, ordering, etc.)
- **Accent Detection:** Italian exercises detect and provide feedback on accent errors (Ã¨ vs Ã©)
- **Common Mistakes Detection:** Unscramble exercises now include pattern-based common mistakes detection with specific feedback messages
- **Cultural Notes:** Info cards support cultural note mode with enhanced styling and contextual information
- **Vocabulary Review:** Match exercises support review mode for vocabulary recycling
- **Extension Activities:** Optional challenge exercises for advanced learners with skip functionality

### UI/UX Enhancements

- **Accented Letter Chips:** Clickable accented letters (Ã , Ã¨, Ã©, Ã¬, Ã², Ã¹) for Italian input in text fields
- **Fade Animations:** Smooth transitions between Boss Fight rounds
- **Progress Tracking:** Visual progress bars and score tracking
- **Hints System:** Real-time word/phrase tracking in Boss Fight with dynamic instruction text
- **Custom Completion Screens:** Self-assessment lessons show custom completion message without score display
- **Consistent Navigation:** All lessons use "Continue to Next Lesson" button for consistent user flow
- **TTS Error Handling:** Graceful fallback to TTS when audio files are unavailable, with user-friendly error messages

### Voice Integration (Current)

- **Whisper STT:** Integrated for Echo Chamber and Boss Fight voice input
- **Edge-TTS:** Currently used for all audio playback, vocabulary pronunciation, and listening comprehension exercises. **Planned for removal in V2.0.**
- **TTS-Based Listening:** Listening comprehension exercises use TTS to read hidden sentences (not displayed to user) with replay functionality
- **Audio Recording:** Browser MediaRecorder API with playback controls
- **Pronunciation Feedback:** Phonetic scoring using Levenshtein distance algorithm
- **Error Handling:** Automatic TTS fallback when audio files fail to load, ensuring exercises remain functional

---

## Scenario-Based Practice Mode (Planned - V2.0)

### Architecture Overview

**Status:** ðŸŽ¯ Designed - Ready for Implementation

The Practice Mode (replacing "Practise (AI)") implements a **Game State Architecture** rather than a simple chatbot, solving the "blank page problem" where learners don't know what to say to an open-ended AI tutor.

### Core Design Principles

1. **Scenario-Based Learning:** Users engage in realistic, contextual conversations (e.g., "Ordering Coffee at a CafÃ©", "Buying a Train Ticket") rather than free-form chat
2. **Mode Separation:** Strict separation between **Voice Mode** (ðŸŽ™ï¸) and **Text Mode** (ðŸ’¬) to prevent cognitive interference (Redundancy Effect)
3. **Game Loop Architecture:** Structured conversation flow with clear win conditions and feedback phases

### Technical Implementation

#### 1. Stage Manager Pattern (Topic Control)

**Problem:** LLMs naturally drift off-topic during roleplay conversations.

**Solution:** Structured System Prompt that acts as a "Stage Manager" defining:
- **Current Scene:** Context and setting
- **Winning Condition:** Goal that ends the interaction
- **Constraints:** Response length, topic boundaries

**Example System Prompt:**
```
"You are roleplaying as a Barista in a busy Italian cafÃ©.
Current Scene: The user is a customer ordering a cappuccino and a cornetto.
Your Goal: Take their order, ask if they want it 'al banco' (at the bar) or 'al tavolo' (at the table), and calculate the total price.
Constraints: Keep responses short (under 20 words). If the user changes the topic (e.g., asks about the weather), politely bring them back to the order."
```

**Drift Prevention:** Backend appends invisible reminder to prompt history before LLM call:
```
[System Note: focus on the coffee order; do not discuss unrelated topics.]
```

#### 2. Goal Check Classifier (Ending Detection)

**Problem:** LLMs never naturally "stop" talking - conversations can continue indefinitely.

**Solution:** "Hidden Thought" Method - Request structured JSON output from Llama 3 Instruct containing both internal state and external speech.

**JSON Response Format:**
```json
{
  "thought": "The user has successfully ordered and paid. The interaction is complete.",
  "scene_status": "COMPLETE",
  "reply": "Ecco a lei. Buona giornata!"
}
```

**Backend Logic:**
- Parse JSON response
- If `scene_status == "COMPLETE"`: Display success modal, end voice recording, trigger feedback phase
- If `scene_status == "ACTIVE"`: Continue conversation loop

#### 3. Post-Game Report (Feedback System)

**Design Philosophy:** No real-time corrections during roleplay (breaks immersion). Feedback provided after conversation completes, like a video game "Mission Report."

**Feedback Architecture:**

**Pronunciation Analysis:**
- Uses Whisper STT confidence scores from client-side
- Levenshtein distance metric for phonetic scoring
- Flags low-confidence transcriptions (e.g., "connetto" instead of "cornetto")

**Grammar & Vocabulary Review:**
- Entire conversation transcript sent to Llama 3 with review prompt:
  ```
  "Review this transcript. Find 3 grammatical errors and suggest 2 better vocabulary words for a beginner learner."
  ```
- Separate system prompt for feedback generation (different from roleplay prompt)

**Contextual Vocabulary Introduction:**
- Scenario-specific vocabulary priming via prompt injection
- Example: "Buying a Train Ticket" scenario ensures words like "binario" (platform) and "biglietto" (ticket) appear naturally
- LLM can explain vocabulary within context when user asks, improving retention vs. flashcards

#### 4. Voice Mode vs. Text Mode Separation

**Cognitive Science Rationale:** Mixing reading/typing with listening/speaking triggers "Redundancy Effect" - learners default to reading when text is visible, stopping active listening practice.

**UI Implementation:**
- **Toggle Switch:** `[ ðŸŽ™ï¸ Voice Mode ] â†” [ ðŸ’¬ Text Mode ]` at top of Practice screen
- **Mode Isolation:** Each mode has distinct interface and backend endpoints

**Technical Architecture:**

**Voice Mode Endpoint** (`/api/voice-chat`):
- **Input:** Audio file (MediaRecorder API)
- **Process:** Whisper STT â†’ Llama 3 (with Game State) â†’ StyleTTS 2 TTS
- **Output:** Audio file
- **GPU Operations:** STT + LLM + TTS (expensive, isolated)

**Text Mode Endpoint** (`/api/text-chat`):
- **Input:** Text string
- **Process:** Llama 3 only (with Game State)
- **Output:** Text string
- **GPU Operations:** LLM only (cheap, fast)

**Transcript Bridge:**
- When Voice Mode conversation completes, transcript automatically saved
- User switches to Text Mode â†’ System message: "Here is the transcript of your voice conversation. Review your mistakes below."
- Enables review without breaking mode separation

### Game Loop Flow

```
1. User Speaks â†’ Whisper STT (transcription)
2. Backend Analysis â†’ Check if "Winning Condition" met (via JSON scene_status)
3. Response Generation:
   - If COMPLETE: Generate closing line + Generate "Mission Report" (Feedback)
   - If ACTIVE: Generate Character Response
4. Audio Playback â†’ StyleTTS 2 TTS (Voice Mode) or Text Display (Text Mode)
5. Loop continues until scene_status == "COMPLETE"
6. Post-Game Report displayed with pronunciation, grammar, and vocabulary feedback
```

### Push-to-Talk Implementation

**Component:** `PushToTalkButton` (React)
- **Hold-to-Record:** MediaRecorder API (existing implementation)
- **Visual Feedback:** Button state changes during recording
- **Release-to-Send:** Audio sent to `/api/voice-chat` endpoint
- **Walkie-Talkie UX:** One-way communication prevents interruption

---

## Roadmap: Next Steps (Priority)

1. **Voice Stack Migration (Priority):**
    
    **1.1 Generate/Record Reference Audio (Pre-requisite):**
    
    - Record or source 3 clean .wav files (5-10s each):
        - `tutor_guide.wav`: Neutral, warm voice (Bilingual Guide)
        - `boss_neighbor_informal.wav`: Friendly, casual voice (for A1.1 Boss)
        - `boss_neighbor_formal.wav`: Polite, older voice (for A1.1 Boss)
    - **Sources:** Self-recording (Best), LibriVox (Public Domain), or Mozilla Common Voice (CC0)
    
    **1.2 Backend Refactor (The "Model Manager"):**
    
    - Draft the `ModelManager` Python class to orchestrate Piper (CPU) and StyleTTS 2 (GPU) alongside Llama 3
    - Implement the `switch_voice(reference_wav)` function to allow dynamic character changing (Old/Young/Male/Female) using StyleTTS 2
    - Handle GPU resource allocation and model switching logic
    
    **1.3 Voice Stack Migration:**
    
    - Remove `edge-tts` dependencies from `requirements.txt`
    - Deploy **Piper TTS** for all static content strings (vocabulary, phrases, exercise instructions)
    - Deploy **StyleTTS 2** for the "Tutor" persona and dynamic AI responses
    - Update all TTS endpoints to use the new ModelManager
    - Test voice cloning with reference audio files
    
2. **Streaming Pipeline Implementation:**
    
    - Implement streaming text generation using `TextIteratorStreamer` from transformers
    - Parse LLM output sentence-by-sentence (split on periods)
    - Stream complete sentences immediately to TTS as they're generated
    - Stream TTS audio chunks back to client as they're produced
    - Target: Reduce perceived latency from ~6 seconds to ~1.5 seconds
    
3. **Scenario-Based Practice Mode Implementation:**
    
    - Implement Game State Architecture in FastAPI backend
        
    - Create Stage Manager Pattern system prompts
        
    - Implement Goal Check Classifier with JSON response parsing
        
    - Build Post-Game Report feedback system
        
    - Develop Voice/Text Mode toggle UI
        
    - Create PushToTalkButton component
        
    - Design scenario library (e.g., "Ordering Coffee", "Buying Train Ticket")
    - Replace the current free tutor mode with this new mode and design
        
4. **âœ… Core Curriculum Complete:** All **10-Module A1 Course** content is pedagogically correct and complete. All modules (A1.1 through A1.10) are fully implemented with structured lesson data, exercise types, and Boss Fight scenarios.
    
6. **Cloud Migration ("The Hub"):**
    
    - **Backend:** Deploy Dockerized FastAPI to **RunPod Serverless GPU**.
        
    - **Database:** Migrate local MongoDB to **MongoDB Atlas (Free Tier)**.
        
    - **Frontend:** Deploy React app to **Vercel/Netlify**.
        
7. **Mobile App Conversion:** Package the React frontend into a native container using **Capacitor**.
    
    - Use `npx cap add android` and `npx cap add ios` to create native projects pointing to the cloud backend (The Hub).
        
8. **Code Cleanup:** Resolve Pydantic V2 and FastAPI `lifespan` deprecation warnings identified during testing.

9. **Global Expansion (Multi-Language Support):**
    
    **Architecture:** Support every language on earth while only paying for one single GPU using MultiLLMManager logic.
    
    **Summary Checklist for Global Expansion:**
    
    - [ ] **Add Storage:** Attach a 50GB Network Volume to your RunPod instance.
    - [ ] **Download Models:**
        - [ ] Llama-3-8B (Euro) - For European languages (Italian, Spanish, French, German, etc.)
        - [ ] Qwen-2.5-7B (Asian) - For Asian languages (Chinese, Japanese, Korean, etc.)
        - [ ] Aya-Expanse-8B (African) - For African languages and other global languages
    - [ ] **Update Backend:** Use the MultiLLMManager logic to swap models based on the user's selected language.
    
    **Technical Implementation:**
    - Model Manager class handles dynamic model loading/unloading based on user language selection
    - Language detection from user profile or explicit selection
    - VRAM management for efficient model swapping
    - Fallback logic if selected model unavailable
    - Language-specific curriculum modules (e.g., `a1_1_module_data_es.py` for Spanish, `a1_1_module_data_fr.py` for French)

---

## Technical Notes

### Model Quantization

- **GPTQ Implementation:** Uses `auto-gptq` library with transformers' auto-detection
- **Model Loading:** `AutoModelForCausalLM.from_pretrained()` automatically detects GPTQ format when `auto-gptq` is installed
- **CUDA Extensions:** Optional pre-built wheels for CUDA 11.8/12.1 improve performance
- **Fallback:** Gracefully falls back to standard model loading if GPTQ unavailable
- **Configuration:** Model name configurable via `MODEL_NAME` environment variable

### Practice Mode Architecture

- **Game State Management:** Conversation state tracked in backend with scene_status, win conditions, and conversation history
- **System Prompt Templates:** Structured prompts for Stage Manager Pattern stored in backend configuration
- **JSON Response Parsing:** Llama 3 Instruct configured to return structured JSON for scene_status detection
- **Feedback Generation:** Separate LLM calls with review-focused prompts for Post-Game Report generation
- **Mode Separation:** Frontend state management ensures Voice and Text modes never mix in UI

### Voice Stack Architecture (Planned)

- **ModelManager Class:** Centralized orchestration of Piper TTS (CPU) and StyleTTS 2 (GPU)
- **Voice Switching:** Dynamic character voice changes via `switch_voice(reference_wav)` function
- **Resource Management:** GPU allocation for StyleTTS 2, CPU allocation for Piper TTS
- **Reference Audio:** 5-10 second .wav files for zero-shot voice cloning

### Data Structure

- **Module Data:** Structured in `backend/a1_1_module_data.py` with hierarchical lesson/exercise organization
- **Exercise Validation:** Context-aware validation logic in `frontend/src/App.jsx` with fallback mechanisms
- **Boss Fight:** Static conversation flow with hard-coded messages and validation rules
- **New Exercise Fields:** 
  - `audio_text`: Hidden sentence for listening comprehension (read by TTS, not displayed)
  - `common_mistakes`: Array of pattern-based mistake detection with explanations
  - `review`: Boolean flag for vocabulary review exercises
  - `extension`: Boolean flag for optional extension activities
  - `cultural_note`: Boolean flag for cultural information cards
  - `required_elements`: Array for free writing validation
  - `form_fields`: Array of field definitions for form fill exercises

### Key Files

- `frontend/src/App.jsx` - Main React application with all exercise components
- `frontend/src/components/PushToTalkButton.jsx` - Voice Mode recording component (to be created)
- `backend/server.py` - FastAPI backend with lesson endpoints and voice processing
- `backend/practice_mode.py` - Scenario-based practice mode logic (to be created)
- `backend/scenario_templates.py` - System prompt templates for scenarios (to be created)
- `backend/a1_1_module_data.py` through `backend/a1_10_module_data.py` - Complete A1 curriculum structured data (10 modules)
- `backend/scripts/seed_a1_1_module.py` through `backend/scripts/seed_a1_10_module.py` - MongoDB seeding scripts for all modules
- `backend/requirements.txt` - Python dependencies (includes `auto-gptq>=0.7.0`, `edge-tts` to be removed)
- `backend/Dockerfile` - Docker configuration with GPTQ CUDA extension installation
- `context-docs/course-docs/The PolyBot A1 Curriculum Master Reference.md` - Curriculum reference

